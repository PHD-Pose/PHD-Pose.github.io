<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PHD</title>
    <link rel="icon" href="images/phd.png">
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            /* Updated background to a white to light-gray gradient */
            background: linear-gradient(to bottom, #FFFFFF, #E5E7EB);
            background-attachment: fixed; /* Ensures the gradient covers the whole page on scroll */
        }
        .section-title {
            border-bottom: 2px solid #e5e7eb;
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
        }
        .bibtex {
            background-color: #f1f1f1;
            border: 1px solid #ddd;
            padding: 1rem;
            border-radius: 8px;
            font-family: monospace;
            white-space: pre-wrap;
            word-wrap: break-word;
            font-size: 0.875rem;
            color: #333;
        }
    </style>
</head>
<body class="text-gray-800">

    <div class="container mx-auto px-4 py-8 md:py-12 max-w-5xl">

        <!-- Paper Title -->
        <header class="text-center mb-8">
            <img src="images/phd.png" alt="PHD Logo" class="w-auto h-32 mx-auto mb-2" onerror="this.onerror=null;this.src='https://placehold.co/100x50/ffffff/333?text=Logo+1';">
            <h1 class="text-4xl md:text-5xl font-bold text-gray-900">
                PHD <br>
                <span class="text-3xl md:text-4xl font-semibold"><u>P</u>ersonalized 3D <u>H</u>uman Body Fitting with Point <u>D</u>iffusion</span>
            </h1>
            <p class="text-2xl text-gray-600 mt-4">ICCV 2025</p>
        </header>

        <!-- Authors -->
        <div class="text-center mb-4 text-lg space-y-2">
            <!-- Row 1: 2 authors -->
            <div class="flex justify-center items-center gap-x-4">
                <span class="whitespace-nowrap"><a href="#" class="font-medium text-blue-600 hover:underline">Hsuan-I Ho</a><sup>1</sup></span>
                <span class="whitespace-nowrap"><a href="#" class="font-medium text-blue-600 hover:underline">Chen Guo</a><sup>1</sup></span>
            </div>
            <!-- Row 2: 5 authors -->
            <div class="flex flex-wrap justify-center items-center gap-x-4">
                <span class="whitespace-nowrap"><a href="#" class="font-medium text-blue-600 hover:underline">Po-Chen Wu</a><sup>3</sup></span>
                <span class="whitespace-nowrap"><a href="#" class="font-medium text-blue-600 hover:underline">Ivan Shugurov</a><sup>3</sup></span>
                <span class="whitespace-nowrap"><a href="#" class="font-medium text-blue-600 hover:underline">Chengcheng Tang</a><sup>3</sup></span>
                <span class="whitespace-nowrap"><a href="#" class="font-medium text-blue-600 hover:underline">Abhay Mittal</a><sup>3</sup></span>
                <span class="whitespace-nowrap"><a href="#" class="font-medium text-blue-600 hover:underline">Sizhe An</a><sup>3</sup></span>
            </div>
            <!-- Row 3: 2 authors -->
            <div class="flex justify-center items-center gap-x-4">
                <span class="whitespace-nowrap"><a href="#" class="font-medium text-blue-600 hover:underline">Manuel Kaufmann</a><sup>1,2†</sup></span>
                <span class="whitespace-nowrap"><a href="#" class="font-medium text-blue-600 hover:underline">Linguang Zhang</a><sup>3†</sup></span>
            </div>
        </div>

        <!-- Equal Advisory Note -->
        <div class="text-center text-sm text-gray-500 mb-8">
            <span>(<sup>†</sup>Equal Advisory)</span>
        </div>

        <!-- Logos and Affiliations -->
        <div class="flex justify-center items-start gap-x-12 my-8">
            <!-- Institute 1 -->
            <div class="text-center">
                <img src="images/ait_logo.png" alt="ETH Zürich Logo" class="w-auto h-16 mx-auto mb-2" onerror="this.onerror=null;this.src='https://placehold.co/100x50/ffffff/333?text=Logo+1';">
                <span class="text-xl text-gray-500"><sup>1</sup>ETH Zürich</span>
            </div>
            <!-- Institute 2 -->
            <div class="text-center">
                <img src="images/aicenter.png" alt="AI center Logo" class="w-auto h-16 mx-auto mb-2" onerror="this.onerror=null;this.src='https://placehold.co/100x50/ffffff/333?text=Logo+2';">
                <span class="text-xl text-gray-500"><sup>2</sup>ETH AI Center</span>
            </div>
            <!-- Institute 3 -->
            <div class="text-center">
                <img src="images/metalogo.png" alt="Meta Logo" class="w-auto h-16 mx-auto mb-2" onerror="this.onerror=null;this.src='https://placehold.co/100x50/ffffff/333?text=Logo+3';">
                <span class="text-xl text-gray-500"><sup>3</sup>Meta</span>
            </div>
        </div>
        

        <!-- Links -->
        <div class="flex justify-center items-center space-x-4 md:space-x-6 mb-12">
            <a href="#" class="flex items-center space-x-2 bg-gray-200 hover:bg-gray-300 text-gray-800 font-semibold py-2 px-4 rounded-lg transition duration-300">
                <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                <span>Paper</span>
            </a>
            <a href="#" class="flex items-center space-x-2 bg-gray-200 hover:bg-gray-300 text-gray-800 font-semibold py-2 px-4 rounded-lg transition duration-300">
                <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg>
                <span>Code</span>
            </a>
            <a href="#" class="flex items-center space-x-2 bg-gray-200 hover:bg-gray-300 text-gray-800 font-semibold py-2 px-4 rounded-lg transition duration-300">
                 <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polygon points="23 7 16 12 23 17 23 7"></polygon><rect x="1" y="5" width="15" height="14" rx="2" ry="2"></rect></svg>
                <span>Video</span>
            </a>
        </div>


        <!-- tl;dr Section -->
        <section class="mb-12">
            <h2 class="text-3xl font-bold section-title">TL;DR</h2>
            <ul class="list-disc list-inside space-y-2 text-lg">
                <li> We introduce a new paradigm of <b>personalized</b> 3D pose estimation tailored for future perceptive AI.</li>
                <li> We design an approach, <b>SHAPify</b>, for calibrating personal shape information from 2D images.</li>
                <li> We propose <b>PointDiT</b>, a novel point diffusion model for 3D pose fitting leveraging personal shape conditions. </li>
                <li> Our method is a <b>plug-and-play</b> module that can enhance the performance of existing 3D pose estimators.</li>

            </ul>
        </section>



        <!-- Teaser Video/Image -->
        <section class="mb-12">
            <div class="bg-white rounded-lg overflow-hidden shadow-lg border border-gray-200">
                <!-- Replace with your video or GIF -->
                <iframe 
                    class="w-full aspect-video"
                    src="https://www.youtube.com/embed/Ujd7pOn8YtM?si=4gcLep3daePrNPB0" 
                    title="YouTube video player" 
                    frameborder="0" 
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
                    referrerpolicy="strict-origin-when-cross-origin" 
                    allowfullscreen></iframe>
            </div>
        </section>

        <!-- Abstract -->
        <section class="mb-12">
            <h2 class="text-3xl font-bold section-title">Abstract</h2>
            <p class="text-lg leading-relaxed text-justify">
                We introduce PHD, a novel approach for personalized 3D human mesh recovery (HMR) and body fitting that leverages user-specific shape information to improve pose estimation accuracy from videos.
                Traditional HMR methods are designed to be user-agnostic and optimized for generalization. While these methods often refine poses using constraints derived from the 2D image to improve alignment, this process compromises 3D accuracy by failing to jointly account for person-specific body shapes and the plausibility of 3D poses.
                In contrast, our pipeline decouples this process by first calibrating the user's body shape and then employing a personalized pose fitting process conditioned on that shape.
                To achieve this, we develop a body shape-conditioned 3D pose prior, implemented as a Point Diffusion Transformer, which iteratively guides the pose fitting via a Point Distillation Sampling loss.
                This learned 3D pose prior effectively mitigates errors arising from an over-reliance on 2D constraints.
                Consequently, our approach improves not only pelvis-aligned pose accuracy but also absolute pose accuracy -- an important metric often overlooked by prior work.
                Furthermore, our method is highly data-efficient, requiring only synthetic data for training, and serves as a versatile plug-and-play module that can be seamlessly integrated with existing 3D pose estimators to enhance their performance.   
            </p>
        </section>

        <!-- Method -->
       <section class="mb-12">
            <h2 class="text-3xl font-bold section-title">Method</h2>

            <!-- Subsection 1: Overall Pipeline -->
            <h3 class="text-2xl font-semibold mb-4">SHAPify</h3>
            <p class="text-lg leading-relaxed mb-8 text-justify">
                We estimate a person's 3D body shape from a single image in a reference pose. Our method works by minimizing the 2D keypoint reprojection error, given a known camera focal length. To resolve the ambiguity of the unknown camera pitch angle, we regularize the optimization with minimal personal measurements, ensuring an accurate and optimal body shape.
            </p>
            <div class="bg-white p-4 rounded-lg border border-gray-200 shadow-md mb-8">
                 <img src="images/shapify.jpg" alt="Method Diagram" class="w-full h-auto rounded">
                 <p class="text-center text-gray-600 mt-4 text-sm"> SHAPify estimate a person's 3D body shape from a single image. The problem is ill-posed because the camera pitch angle is unknown. We resolve this by using minimal body height and weight as regularization in the optimization process.</p>
            </div>

            <!-- Subsection 2: Point Diffusion Model -->
            <h3 class="text-2xl font-semibold mb-4">Shape-Conditioned Point Diffusion Transformer</h3>
            <p class="text-lg leading-relaxed mb-8 text-justify">
                To facilitate personalized 3D pose fitting, we propose PointDiT, a novel point diffusion transformer that generates 3D human poses conditioned on both the input image and the individual's body shape.
                PointDiT employs a rectified flow formulation to iteratively denoise random point clouds in as few as 5 denoising steps. 
                This approach enables the generation of plausible 3D poses that align well with the observed 2D data while respecting the unique body shape of the individual.
            </p>
            <div class="bg-white p-4 rounded-lg border border-gray-200 shadow-md mb-8">
                 <img src="images/PointDiT.jpg" alt="Method Diagram" class="w-full h-auto rounded">
                 <p class="text-center text-gray-600 mt-4 text-sm"> PointDiT is a novel point diffusion transformer leveraging the rectified flow formulation. It samples 3D body surface points by iteratively denoising random point clouds, conditioned on the image tokens and individual's body shape.</p>
            </div>

            <!-- Subsection 3: Point Distilled Body Fitting -->
            <h3 class="text-2xl font-semibold mb-4">Point Distilled Body Fitting</h3>
            <p class="text-lg leading-relaxed mb-8 text-justify">
                The learned PointDiT model servers as a powerful 3D pose prior for guiding the body fitting and mitigating errors from over-reliance on 2D constraints.
                Inspired by Score Distillation Sampling, we introduce a Point Distillation Sampling loss that leverages PointDiT to refine 3D pose fitting.
            </p>
            <div class="bg-white p-4 rounded-lg border border-gray-200 shadow-md mb-8">
                 <img src="images/fitting.jpg" alt="Method Diagram" class="w-full h-auto rounded">
                 <p class="text-center text-gray-600 mt-4 text-sm"> The Point Distillation Sampling loss guides 3D pose fitting leveraging the capabilities of PointDiT, ensuring the incorporation of personal body shape information and preventing overfitting to 2D keypoints.</p>
            </div>
        </section>

        <!-- Results -->
        <section class="mb-12">
            <h2 class="text-3xl font-bold section-title">Results</h2>

            <h3 class="text-2xl font-semibold mb-4">3D Pose Fitting Accuracy</h3>
            <p class="text-lg leading-relaxed mb-8 text-justify">
                We compare our approach to ScoreHMR, a recent diffusion-based pose fitting method which also leverages a body prior but still relies heavily on regressors for initialization.
            </p>
            <div class="space-y-12">
                <!-- Result Item 2 -->
                <div class="bg-white rounded-lg overflow-hidden border border-gray-200">
                    <img src="images/main_res.jpg" alt="Result 2" class="w-full h-auto">
                    <p class="p-4 text-gray-600 text-center">Comparisons of sampling poses and body fitting with ScoreHMR. Our method matches both 2D images and 3D ground truth (grey color meshes) better</p>
                </div>

                <!-- Result Item 1 -->
                <div class="bg-white rounded-lg overflow-hidden border border-gray-200">
                    <video class="w-full h-auto" controls autoplay muted loop>
                        <source src="videos/result1.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="p-4 text-gray-600 text-center">While the 2D reprojections of ScoreHMR appear reasonable, it struggles to produce consistent and accurate shapes over time. In 3D view, ScoreHMR also fails to predict correct and stable 3D poses.</p>
                </div>

            </div>
        </section>
        
        <!-- References -->
        <section class="mb-12">
            <h2 class="text-3xl font-bold section-title">References</h2>
            <div class="space-y-4 text-gray-700 text-lg">
                <p>[1] Stathopoulos et al. (CVPR 2024). <a href="https://statho.github.io/ScoreHMR/" class="text-blue-600 hover:underline">Score-Guided Diffusion for 3D Human Recovery</a>.</p>
                <p>[2] Patel et al. (3DV 2025). <a href="https://camerahmr.is.tue.mpg.de/" class="text-blue-600 hover:underline">CameraHMR: Aligning People with Perspective</a>.</p>
                <p>[3] Goel et al. (ICCV 2023). <a href="https://shubham-goel.github.io/4dhumans/" class="text-blue-600 hover:underline">Humans in 4D: Reconstructing and Tracking Humans with Transformers</a>.</p>
                <p>[4] Choutas et al. (CVPR 2022). <a href="https://shapy.is.tue.mpg.de/" class="text-blue-600 hover:underline">Accurate 3D Body Shape Regression using Metric and Semantic Attribute</a>.</p>
                <p>[5] Black et al. (CVPR 2023). <a href="https://bedlam.is.tuebingen.mpg.de/" class="text-blue-600 hover:underline">BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion</a>.</p>
                <p>[6] Kocabas et al. (ICCV 2021). <a href="https://spec.is.tue.mpg.de/" class="text-blue-600 hover:underline">SPEC: Seeing People in the Wild with an Estimated Camera</a>.</p>
            </div>
        </section>
        

        <!-- Citation -->
        <section>
            <h2 class="text-3xl font-bold section-title">Citation</h2>
            <p class="text-lg leading-relaxed mb-4">If you find our work useful, please consider citing:</p>
            <div class="bibtex">
@inproceedings{ho2025phd,
    title={PHD: Personalized 3D Human Body Fitting with Point Diffusion},
    author={Ho, Hsuan-I and Guo, Chen and Wu, Po-Chen and Shugurov, Ivan and Tang, Chengcheng and Mittal, Abhay and An, Sizhe and Kaufmann, Manuel and Zhang, Linguang}, 
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    year={2025}
}
            </div>
        </section>

        <!-- Footer -->
        <footer class="text-center mt-12 pt-8 border-t border-gray-300">
            <p class="text-gray-500">Template generated by Gemini inspired by modern academic project pages.</p>
        </footer>

    </div>

</body>
</html>